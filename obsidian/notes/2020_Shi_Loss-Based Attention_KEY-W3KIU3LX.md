# Loss-Based Attention for Deep Multiple Instance Learning
> [!info]+ <center>Metadata</center>
> 
> |<div style="width: 5em">Key</div>|Value|
> |--:|:--|
> |æ–‡çŒ®ç±»å‹|journalArticle|
> |æ ‡é¢˜|Loss-Based Attention for Deep Multiple Instance Learning|
> |çŸ­æ ‡é¢˜||
> |ä½œè€…|[[Xiaoshuang Shi]]ã€ [[Fuyong Xing]]ã€ [[Yuanpu Xie]]ã€ [[Zizhao Zhang]]ã€ [[Lei Cui]]ã€ [[Lin Yang]]|
> |æœŸåˆŠåç§°|[[Proceedings of the AAAI Conference on Artificial Intelligence]]|
> |DOI|[10.1609/aaai.v34i04.6030](https://doi.org/10.1609/aaai.v34i04.6030)|
> |å­˜æ¡£ä½ç½®||
> |é¦†è—ç›®å½•||
> |ç´¢ä¹¦å·||
> |ç‰ˆæƒ|Copyright (c) 2020 Association for the Advancement of Artificial Intelligence|
> |åˆ†ç±»|[[00_Histology, 202204, MIL]]|
> |æ¡ç›®é“¾æ¥|[My Library](zotero://select/library/items/W3KIU3LX)|
> |PDF é™„ä»¶|<ul><li><a href="zotero://open-pdf/library/items/3P7B674A">Full Text PDF</a></li><li><a href="zotero://open-pdf/library/items/EKX2QURQ">zhao2020.pdf</a></li></ul>|
> |å…³è”æ–‡çŒ®|[[2018_Ilse_Attention-based Deep_KEY-GMXKWTZJ]]ã€ [[2020_Lu_Data Efficient and W_KEY-R8BCS375]]|
> ^Metadata


> [!example]- <center>æœ¬æ–‡æ ‡ç­¾</center>
> 
> `$=dv.current().file.tags`


> [!quote]- <center>Abstract</center>
> 
> Although attention mechanisms have been widely used in deep learning for many tasks, they are rarely utilized to solve multiple instance learning (MIL) problems, where only a general category label is given for multiple instances contained in one bag. Additionally, previous deep MIL methods firstly utilize the attention mechanism to learn instance weights and then employ a fully connected layer to predict the bag label, so that the bag prediction is largely determined by the effectiveness of learned instance weights. To alleviate this issue, in this paper, we propose a novel loss based attention mechanism, which simultaneously learns instance weights and predictions, and bag predictions for deep multiple instance learning. Specifically, it calculates instance weights based on the loss function, e.g. softmax+cross-entropy, and shares the parameters with the fully connected layer, which is to predict instance and bag predictions. Additionally, a regularization term consisting of learned weights and cross-entropy functions is utilized to boost the recall of instances, and a consistency cost is used to smooth the training process of neural networks for boosting the model generalization performance. Extensive experiments on multiple types of benchmark databases demonstrate that the proposed attention mechanism is a general, effective and efficient framework, which can achieve superior bag and image classification performance over other state-of-the-art MIL methods, with obtaining higher instance precision and recall than previous attention mechanisms. Source codes are available on https://github.com/xsshi2015/Loss-Attention.


> [!tldr]- <center>éšè—ä¿¡æ¯</center>
> 
> itemType:: journalArticle
> title:: Loss-Based Attention for Deep Multiple Instance Learning
> shortTitle:: 
> creators:: [[Xiaoshuang Shi]]ã€ [[Fuyong Xing]]ã€ [[Yuanpu Xie]]ã€ [[Zizhao Zhang]]ã€ [[Lei Cui]]ã€ [[Lin Yang]]
> publicationTitle:: [[Proceedings of the AAAI Conference on Artificial Intelligence]]
> journalAbbreviation:: 
> volume:: 34
> issue:: 04
> pages:: 5742-5749
> language:: en
> DOI:: [10.1609/aaai.v34i04.6030](https://doi.org/10.1609/aaai.v34i04.6030)
> ISSN:: 2374-3468
> url:: [https://ojs.aaai.org/index.php/AAAI/article/view/6030](https://ojs.aaai.org/index.php/AAAI/article/view/6030)
> archive:: 2022-07-25
> archiveLocation:: 
> libraryCatalog:: 
> callNumber:: 
> rights:: Copyright (c) 2020 Association for the Advancement of Artificial Intelligence
> extra:: ğŸ·ï¸ _é¡¶ä¼šã€/Doneã€ğŸ“’
> collection:: [[00_Histology, 202204, MIL]]
> tags:: #_é¡¶ä¼š #Done 
> related:: [[2018_Ilse_Attention-based Deep_KEY-GMXKWTZJ]]ã€ [[2020_Lu_Data Efficient and W_KEY-R8BCS375]]
> itemLink:: [My Library](zotero://select/library/items/W3KIU3LX)
> pdfLink:: <ul><li><a href="zotero://open-pdf/library/items/3P7B674A">Full Text PDF</a></li><li><a href="zotero://open-pdf/library/items/EKX2QURQ">zhao2020.pdf</a></li></ul>
> qnkey:: 2020_Shi_Loss-Based Attention_KEY-W3KIU3LX
> date:: 2020-04-03
> dateY:: 2020
> dateAdded:: 2022-04-25
> dateModified:: 2022-07-25
> year:: 2022
> dateCurrent:: 2022-08-01
> time:: 10:34:17
> week:: æ˜ŸæœŸä¸€
> yearMonth:: 2022-08
> dateWeek:: 2022-08-01 æ˜ŸæœŸä¸€
> dateTime:: 2022-08-01 10:34:17
> dateWeekTime:: 2022-08-01 10:34:17 æ˜ŸæœŸä¸€
> 
> abstract:: Although attention mechanisms have been widely used in deep learning for many tasks, they are rarely utilized to solve multiple instance learning (MIL) problems, where only a general category label is given for multiple instances contained in one bag. Additionally, previous deep MIL methods firstly utilize the attention mechanism to learn instance weights and then employ a fully connected layer to predict the bag label, so that the bag prediction is largely determined by the effectiveness of learned instance weights. To alleviate this issue, in this paper, we propose a novel loss based attention mechanism, which simultaneously learns instance weights and predictions, and bag predictions for deep multiple instance learning. Specifically, it calculates instance weights based on the loss function, e.g. softmax+cross-entropy, and shares the parameters with the fully connected layer, which is to predict instance and bag predictions. Additionally, a regularization term consisting of learned weights and cross-entropy functions is utilized to boost the recall of instances, and a consistency cost is used to smooth the training process of neural networks for boosting the model generalization performance. Extensive experiments on multiple types of benchmark databases demonstrate that the proposed attention mechanism is a general, effective and efficient framework, which can achieve superior bag and image classification performance over other state-of-the-art MIL methods, with obtaining higher instance precision and recall than previous attention mechanisms. Source codes are available on httpsï¼š//github.com/xsshi2015/Loss-Attention.


ğŸ‘£â¿ğŸ‘£


## âœï¸ ç¬”è®°åŒº

>[!inbox]- <center>ğŸ“« ç¬”è®°ç®€æŠ¥</center>
>
> â° importDate:: 2022-07-26

> [!IMPORTANT]+ <center>ğŸŒ± ç ”è¯»å°è±¡</center>  
>
>ğŸ“Œ comment::  

> [!WARNING]+ <center>ğŸ£ æ€»ç»“</center>  
>
>ğŸ¯ ç ”ç©¶é—®é¢˜::  
ğŸ” ç ”ç©¶èƒŒæ™¯::  
ğŸš€ ç ”ç©¶æ–¹æ³•::  
ğŸ” ç ”ç©¶æ€è·¯::  
ğŸ“º ä¸»è¦å†…å®¹::  
ğŸ‰ ç ”ç©¶ç»“è®º::  
ğŸ—ï¸ åˆ›æ–°ç‚¹::  
ğŸ’© ç ”ç©¶å±€é™::  
ğŸ¾ ç ”ç©¶å±•æœ›::  
âœï¸ å¤‡æ³¨::  



ğŸ‘£â¿ğŸ‘£

## ğŸ“ æ³¨é‡Šç¬”è®° 3P7B674A

> <span style="font-size: 15px;color: gray">ğŸ“ 2020-Shi-Loss-Based Attention for Deep Multiple Instance Learning</span>

^KEYrefTitle

> <span class="highlight" style="background-color: #ff666640">Additionally, previous deep MIL methods firstly utilize the attention mechanism to learn instance weights and then employ a fully connected layer to predict the bag label, so that the bag prediction is largely determined by the effectiveness of learned instance weights.</span>  
> ä¹‹å‰çš„attentionMILå¾ˆå¤§ç¨‹åº¦ä¸Šä¼šå—åˆ°å­¦ä¹ åˆ°çš„instanceæœ‰æ•ˆæƒé‡çš„å½±å“ ([p5742](zotero://open-pdf/library/items/3P7B674A?page=5742&annotation=PRIEBFY8))

^KEYPRIEBFY8

> <span class="highlight" style="background-color: #ff666640">simultaneously learns instance weights and predictions</span>  
> åŒæ—¶å­¦ä¹ instanceçš„æƒé‡å’Œé¢„æµ‹ ([p5742](zotero://open-pdf/library/items/3P7B674A?page=5742&annotation=9A6LZ95E))

^KEY9A6LZ95E

> <span class="highlight" style="background-color: #ff666640">Additionally, a regularization term consisting of learned weights and crossentropy functions is utilized to boost the recall of instances, and a consistency cost is used to smooth the training process of neural networks for boosting the model generalization performance.</span> ([p5742](zotero://open-pdf/library/items/3P7B674A?page=5742&annotation=6H47CFN2))

^KEY6H47CFN2

> <span class="highlight" style="background-color: #ffd40040">The main goal of MIL</span>  
> MILçš„ä¸»è¦ç›®æ ‡ ([p5742](zotero://open-pdf/library/items/3P7B674A?page=5742&annotation=KGIX4RXE))

^KEYKGIX4RXE

> <span class="highlight" style="background-color: #ffd40040">expectation-maximization (EM) methods</span> ([p5743](zotero://open-pdf/library/items/3P7B674A?page=5743&annotation=XJ2TGMBX))

^KEYXJ2TGMBX

> <span class="highlight" style="background-color: #ffd40040">n bags</span> ([p5744](zotero://open-pdf/library/items/3P7B674A?page=5744&annotation=64RBM3KD))

^KEY64RBM3KD

> <span class="highlight" style="background-color: #ffd40040">ni instances</span> ([p5744](zotero://open-pdf/library/items/3P7B674A?page=5744&annotation=PZQ9UKZV))

^KEYPZQ9UKZV

> <span class="highlight" style="background-color: #ff666640">hLâˆ’1 i is obtained by a mean operator</span>  
> å¯¹æ‰€æœ‰çš„instanceçš„ç‰¹å¾è¾“å‡ºåšäº†ä¸ªmeanï¼Œç„¶åå†ç”¨ ([p5744](zotero://open-pdf/library/items/3P7B674A?page=5744&annotation=DZQJUZBN))

^KEYDZQJUZBN

> <span class="highlight" style="background-color: #ffd40040">exp(zi,j,k) â‰ˆ âˆ‘Kâˆ’1 c=0 exp(zi,j,c).</span> ([p5745](zotero://open-pdf/library/items/3P7B674A?page=5745&annotation=RHBIVZ7A))

^KEYRHBIVZ7A

> <span class="highlight" style="background-color: #ffd40040">the popular maxpooling operator in Theorem 1</span> ([p5745](zotero://open-pdf/library/items/3P7B674A?page=5745&annotation=SMM5393S))

^KEYSMM5393S

> <span class="highlight" style="background-color: #ff666640">Theorem 1</span>  
> å¯¹æ¯”attentionå’Œmaxæ“ä½œï¼Œå°½ç®¡æ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯attentionçš„ä¼šå¹³æ»‘ ([p5745](zotero://open-pdf/library/items/3P7B674A?page=5745&annotation=77K5DVL4))

^KEY77K5DVL4







